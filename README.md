# Arxiv Abstract Analysis

## Project Goals
Arxiv hosts millions of papers that are classified into hundreds of research categories. It can be a daunting task to understand what kinds of papers belong to which category and how these categories are related. The visualizations performed in this project can be used to answer a multitude of questions including but not limited to –  

1. **How closely are the selected research categories related to each other?** The t-SNE scatterplot can help the users answer this question. It can also help them understand what kind of papers are more closely related to each other. For example, if the user is comparing two categories – `cs.AI` and `stat.ML`, there may some papers in the `cs.AI` cluster that are closer to points in the `stat.ML` cluster. The user can glean insight into why these papers are closer to the other cluster.

2. **While submitting a paper on Arxiv, which category should I select for my paper?** It can be important to choose the right categories while submitting a paper on Arxiv to maximise the visibility of the paper in the relevant research domains. With the help of the wordcloud as well as the t-SNE plot, the user can better understand which category their own paper belongs to.  

## Design Decision Rationale
Having text data, I had to find a way to encode the text as a vector as well as learn a latent, low-dimensional representation in order to visualize it. I also had to find the best way to represent the raw abstract text in every research category.  

For the former, I decided to use a _Scatterplot_ after performing t-SNE dimensionality reduction after representing the abstracts as vectors using Doc2Vec. I gave the user the option to choose the perplexity used to learn the manifold as well as the number of dimensions in the latent representation. The user also has the ability to choose which Arxiv categories to compare. A 2D or a 3D scatterplot is plotted based on the number of latent dimensions. The user is able to zoom into the plot and hover over points to learn more about it. Alternatively, I considered using PCA for dimensionality reduction and average Word2Vec for the vector representations. However, PCA does not work as well as t-SNE for non-linear data and using average Word2Vec instead of Doc2Vec could lead to loss of information.

For the case of representing the raw abstract text in every research category, I chose to go with an interactive wordcloud. A wordcloud is intuitive and is able to pack a lot of information in a small amount of space. The user is given the option to choose the research category and the number of words in the wordcloud. Before plotting the wordcloud, I preprocessed the data by converting it to lowercase, removing punctuations and stopwords, tokenizing and lemmatizing. I used Term Frequency to determine the top-N words in a category. As an alternative, I considered using a Bar Chart for visualization and TF-IDF to rank the words. Bar Charts are not as densely packed with information. Especially as the number of words increase, the Bar Plots become harder to read. Although TF-IDF might have been then better choice, I decided to go with Term Frequency because sklearn's TFIFDFVectorizer was extremely slow and I decided to go with a faster solution, since I already had one visulization that took a long time to compute.

## Development Process
The overall development of this application took me around 15-16 hours. The development can be divided into three phases – Planning, Data Preprocessing and Data Visualization. The latter two phases of development took about the same time, with Data Preprocessing taking slightly longer. I started the development process by searching for a dataset that looked interesting to me. Once I found the Arxiv dataset, I thought about questions I wanted to answer with this dataset, what visualizations I could use to answer these questions and what elements of interactivity I could add to these visualizations. After I had answered these three questions, I preprocessed the data, wrote the code to visualize the processed data and finally deploy my code using Streamlit.

## User Notes:
Since Streamlit does not support Wordclouds out of the box, I used [this library](https://github.com/rezaho/streamlit-wordcloud) for the wordcloud since it provides a bit of interactivity. However, it does have a refresh issue as mentioned [here](https://github.com/rezaho/streamlit-wordcloud/issues/2). If the frequent refresh is bothersome, you can use [this non-interactive library](https://github.com/amueller/word_cloud) instead, with the help of [this example](https://discuss.streamlit.io/t/wordcloud/5714/2). This will create a wordcloud and insert it as an image.